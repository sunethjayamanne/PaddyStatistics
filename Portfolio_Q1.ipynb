{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3760bb71-907b-41e4-83c7-014663bafbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction successful! File saved as 'paddy_statistics_1952_2015'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the target web page\n",
    "url = \"https://www.statistics.gov.lk/Agriculture/StaticalInformation/PaddyStatistics/Annual-BothSeasons-1952-2015\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for request errors\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all tables in the webpage\n",
    "tables = soup.find_all('table')\n",
    "# Find all tables\n",
    "#tables = soup.find_all('table')\n",
    "\n",
    "# Select the correct table (skip the horizontal preview table)\n",
    "table = tables[-1]  # Usually, the last table contains the actual data\n",
    "\n",
    "# Extract table data\n",
    "data = []\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all(['th', 'td'])\n",
    "    cell_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "    # Append only rows that have meaningful data (ignore preview section)\n",
    "    if len(cell_data) > 3:  \n",
    "        data.append(cell_data)\n",
    "\n",
    "max_columns = max(len(row) for row in data)\n",
    "\n",
    "# Standardize row lengths by filling missing values\n",
    "for row in data:\n",
    "    while len(row) < max_columns:\n",
    "        row.append(\"\")  # Fill missing cells with empty values\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally rename columns if the first row is the header\n",
    "df.columns = df.iloc[0]  # Set first row as header\n",
    "df = df[1:]  # Remove first row from data\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.to_csv(\"D:/Personal/MSc/MSc. Data Science/Lecture Notes/2. Principles of DS/Assignment and Passpapers/Assignments/Portfolio/paddy_statistics_1952_2015.csv\", index=False)\n",
    "print(\"Data extraction successful! File saved as 'paddy_statistics_1952_2015'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6044a8c5-7034-4329-be8e-aae07d9f2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data extraction successful! The file 'paddy_statistics_1952_2015.csv' has been saved.\n"
     ]
    }
   ],
   "source": [
    "## This one no need \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the target web page\n",
    "url = \"https://www.statistics.gov.lk/Agriculture/StaticalInformation/PaddyStatistics/Annual-BothSeasons-1952-2015\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for request errors\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all tables in the webpage\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# Extract data if tables exist\n",
    "if tables:\n",
    "    # Extract the first table (assuming the target data is in the first table)\n",
    "    table = tables[0]\n",
    "\n",
    "    # Initialize a list to store the extracted data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over table rows\n",
    "    for row in table.find_all('tr'):\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        cell_data = [cell.get_text(strip=True) for cell in cells]\n",
    "        if cell_data:\n",
    "            data.append(cell_data)\n",
    "\n",
    "    # Ensure all rows have the same number of columns\n",
    "    max_columns = max(len(row) for row in data)\n",
    "    \n",
    "    # Standardize row lengths by padding missing values\n",
    "    for row in data:\n",
    "        while len(row) < max_columns:\n",
    "            row.append(\"\")  \n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the extracted data as a CSV file\n",
    "    df.to_csv(\"D:/Personal/MSc/MSc. Data Science/Lecture Notes/2. Principles of DS/Assignment and Passpapers/Assignments/Portfolio/paddy_statistics_1952_2015_3.csv\", index=False)\n",
    "\n",
    "    print(\"✅ Data extraction successful! The file 'paddy_statistics_1952_2015.csv' has been saved.\")\n",
    "else:\n",
    "    print(\"❌ No tables found on the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ff99-f12a-4ceb-b5c5-15b6326f8be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
